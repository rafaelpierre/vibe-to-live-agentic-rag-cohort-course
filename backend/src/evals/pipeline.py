from src.evals.generate_spans import generate_input_queries, generate_responses
from src.evals.llm_as_judge import evaluate_relevance, annotate_span_evals
from src.evals.data import get_data
import asyncio
import time
from typing import List
import pandas as pd


def run_synthetic_relevance_pipeline(max_queries: int = 20) -> List[str]:
    """Execute the complete end-to-end synthetic evaluation pipeline.

    This function orchestrates the full evaluation workflow for the RAG agent by:
    1. Generating synthetic queries that simulate user questions
    2. Running each query through the agent to generate responses (creating spans in Phoenix)
    3. Waiting for spans to be available in the Phoenix observability platform
    4. Retrieving the created spans from Phoenix
    5. Evaluating response relevance using an LLM-as-a-judge approach
    6. Annotating the spans in Phoenix with evaluation results

    The pipeline is designed to run synthetically (without real user input) for
    automated testing and continuous evaluation of the RAG agent's performance.
    All async operations are handled internally via asyncio.run().

    The function is resilient to Phoenix connectivity issues - if spans cannot be
    retrieved or annotated, it will log warnings and return the responses without
    completing the evaluation steps, rather than failing entirely.

    Args:
        max_queries (int): The number of synthetic queries to generate and evaluate.
            This directly controls the scope of the evaluation run. Defaults to 20.
            Higher values provide more comprehensive evaluation but take longer to run.

    Returns:
        List[str]: A list of response strings generated by the agent, one for each
            synthetic query. The list will have exactly max_queries elements.
            These responses can be used for further analysis or testing.

    Side Effects:
        - Creates spans in Phoenix for each query-response interaction
        - Creates evaluation annotations in Phoenix (if Phoenix is available)
        - Prints status messages about pipeline progress
        - Sleeps for 3 seconds to allow Phoenix to process spans

    Example:
        >>> responses = run_synthetic_relevance_pipeline(max_queries=10)
        Waiting for spans to be available in Phoenix...
        Successfully annotated 10 spans
        >>> print(len(responses))
        10

    Note:
        - This is a synchronous function that wraps async operations internally
        - Requires Phoenix observability platform to be running and accessible
        - Uses a 3-second wait period for span availability (may need adjustment)
        - Falls back gracefully if Phoenix is unavailable
        - Retrieves the most recent spans from Phoenix based on query count
        - Project name is hardcoded as 'fast_api_agent'

    Workflow Details:
        Step 1: Generate synthetic queries using generate_input_queries()
        Step 2: Run each query through get_chat_response() to create spans
        Step 3: Wait 3 seconds for Phoenix to index the spans
        Step 4: Fetch the most recent spans from Phoenix using get_data()
        Step 5: Evaluate relevance using evaluate_relevance() LLM judge
        Step 6: Annotate spans with results using annotate_span_evals()
    """

    async def pipeline() -> List[str]:
        # Step 1: Generate queries and responses (creates spans in Phoenix)
        queries = await generate_input_queries(max_queries=max_queries)
        responses = []
        for query in queries:
            from src.web.services import get_chat_response

            response = await get_chat_response(prompt=query)
            responses.append(str(response))

        # Step 2: Wait for spans to be available in Phoenix
        print("Waiting for spans to be available in Phoenix...")
        time.sleep(3)  # Give Phoenix time to process the spans

        # Step 3: Fetch real spans from Phoenix
        try:
            spans_df = get_data(project_name="fast_api_agent", debug=True)
            # Get the most recent spans (assuming they're the ones we just created)
            recent_spans = spans_df.tail(len(queries)).reset_index()

            # Step 4: Evaluate relevance using real spans
            evaluation_df = await evaluate_relevance(data=recent_spans)

            # Step 5: Annotate spans in Phoenix
            annotate_span_evals(evaluation_results=evaluation_df)

            return responses
        except Exception as e:
            print(f"Warning: Could not fetch spans from Phoenix: {e}")
            print("Returning responses without evaluation")
            return responses

    return asyncio.run(pipeline())
