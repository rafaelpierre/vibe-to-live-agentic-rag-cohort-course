import pandas as pd
from phoenix.evals import llm_classify, OpenAIModel
from phoenix.client import Client
from textwrap import dedent
import os


JUDGE_PROMPT_TEMPLATE = dedent("""
    You are a judge that evaluates the relevance of responses given input queries.
    For each input query and its corresponding output response, rate the response on a scale of 1 to 5,
    where 5 is an excellent response that fully addresses the query, and 1 is a poor response that fails to address the query.
    
    Provide a brief explanation for your rating.
    Input: {input.value}
    AI Response: {output.value}
    Format your response as:
    Rating: <1-5>
    Explanation: <your explanation here>
""")

JUDGE_RAILS = [str(i) for i in range(1, 6)]


async def evaluate_relevance(data: pd.DataFrame) -> pd.DataFrame:
    """Evaluate the relevance of agent responses using an LLM-as-a-judge approach.

    This async function implements the LLM-as-a-judge evaluation pattern, where a
    separate language model (GPT-4.1) evaluates the quality and relevance of responses
    generated by the RAG agent. The judge model rates each response on a 1-5 scale
    and provides explanations for its ratings.

    The function uses Phoenix's llm_classify utility to parallelize evaluations across
    multiple input-output pairs. The judge model is configured with a concurrency of 10
    to efficiently process multiple evaluations in parallel while respecting rate limits.

    The evaluation process:
    1. Each input query and output response pair is formatted using the JUDGE_PROMPT_TEMPLATE
    2. The judge model analyzes the response's relevance to the query
    3. A rating (1-5) and explanation are generated
    4. Results are combined with the original data and indexed by span_id

    Args:
        data (pd.DataFrame): Input DataFrame containing spans to evaluate. Must include:
            - 'context.span_id': Unique identifier for each span
            - 'input.value': The input query or prompt
            - 'output.value': The agent's response to evaluate
            Additional columns are preserved in the output.

    Returns:
        pd.DataFrame: Combined DataFrame with original data plus evaluation results:
            - All original columns from input data
            - 'label': The relevance rating (1-5 as string)
            - 'explanation': The judge's reasoning for the rating
            - Index set to 'context.span_id' for easy lookups

    Example:
        >>> data = pd.DataFrame({
        ...     'context.span_id': ['span_0', 'span_1'],
        ...     'input.value': ['What is inflation?', 'Tell me about rates'],
        ...     'output.value': ['Inflation is...', 'Interest rates are...']
        ... })
        >>> results = await evaluate_relevance(data)
        >>> print(results[['label', 'explanation']])

    Note:
        - This is an async function and must be awaited
        - Uses GPT-4.1 as the judge model (configured via OPENAI_API_ENDPOINT)
        - Supports up to 10 concurrent evaluations
        - Ratings are constrained to ['1', '2', '3', '4', '5'] via JUDGE_RAILS
        - Handles both DataFrame and list return types from llm_classify
    """

    llm_judge_model = OpenAIModel(
        model="gpt-4.1",
        default_concurrency=10,
        base_url=os.getenv("OPENAI_API_ENDPOINT"),
    )

    eval_results = llm_classify(
        data=data,
        template=JUDGE_PROMPT_TEMPLATE,
        model=llm_judge_model,
        provide_explanation=True,
        rails=JUDGE_RAILS,
    )

    # Handle case where eval_results might be a list or other type
    if isinstance(eval_results, pd.DataFrame):
        eval_df = pd.concat([data, eval_results], axis=1).set_index("context.span_id")
    else:
        # If eval_results is not a DataFrame, convert it
        if isinstance(eval_results, list):
            eval_results_df = pd.DataFrame(eval_results)
        else:
            eval_results_df = pd.DataFrame([eval_results])
        eval_df = pd.concat([data, eval_results_df], axis=1).set_index(
            "context.span_id"
        )

    return eval_df


def annotate_span_evals(evaluation_results: pd.DataFrame) -> None:
    """Persist evaluation results back to Phoenix as span annotations.

    This function takes evaluation results (typically from evaluate_relevance) and
    uploads them to the Phoenix observability platform as span annotations. This enables
    viewing evaluation scores and explanations alongside traces in the Phoenix UI,
    facilitating debugging, monitoring, and quality analysis of the RAG agent.

    Annotations are logged with the name "relevance" and marked as coming from an
    LLM annotator, distinguishing them from human annotations or other automated
    evaluations. The function operates synchronously to ensure all annotations are
    persisted before returning.

    Args:
        evaluation_results (pd.DataFrame): DataFrame containing evaluation results with:
            - Index: 'context.span_id' identifying which span to annotate
            - 'label': The evaluation score/rating
            - 'explanation': The reasoning for the evaluation
            Additional columns are ignored but preserved in Phoenix.

    Returns:
        None or List: Returns annotation IDs if successful, None if annotation fails.
            In case of failure, prints a warning but doesn't raise an exception,
            allowing the evaluation pipeline to complete even if Phoenix is unavailable.

    Side Effects:
        - Creates annotations in Phoenix for each row in evaluation_results
        - Prints success message with count of annotated spans
        - Prints warning message if annotation fails

    Example:
        >>> eval_df = pd.DataFrame({
        ...     'label': ['4', '5'],
        ...     'explanation': ['Good response', 'Excellent response']
        ... }, index=pd.Index(['span_0', 'span_1'], name='context.span_id'))
        >>> annotate_span_evals(eval_df)
        Successfully annotated 2 spans

    Note:
        - Requires PHOENIX_COLLECTOR_ENDPOINT and PHOENIX_API_KEY environment variables
        - Uses sync=True to ensure annotations are persisted before returning
        - Gracefully handles errors to prevent pipeline failures
        - Annotations appear in Phoenix UI under the "relevance" annotation name
        - Annotator kind is set to "LLM" to distinguish from human annotations
    """
    try:
        client = Client(
            base_url=os.getenv("PHOENIX_COLLECTOR_ENDPOINT"),
            api_key=os.getenv("PHOENIX_API_KEY"),
        )

        annotation_ids = client.spans.log_span_annotations_dataframe(
            dataframe=evaluation_results,
            annotation_name="relevance",
            annotator_kind="LLM",
            sync=True,
        )

        print(f"Successfully annotated {len(annotation_ids)} spans")
        return annotation_ids
    except Exception as e:
        print(f"Warning: Could not annotate spans in Phoenix: {e}")
        print("Evaluation completed but annotation skipped")
        return None
